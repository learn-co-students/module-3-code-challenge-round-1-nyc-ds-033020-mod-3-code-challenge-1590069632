{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Code Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This assessment is designed to test your understanding of Module 3 material. It covers:\n",
    "\n",
    "* Gradient Descent\n",
    "* Logistic Regression\n",
    "* Classification Metrics\n",
    "* Decision Trees\n",
    "\n",
    "_Read the instructions carefully._ You will be asked both to write code and respond to a few short answer questions.\n",
    "\n",
    "### Note on the short answer questions\n",
    "\n",
    "For the short answer questions, _please use your own words._ The expectation is that you have **not** copied and pasted from an external source, even if you consult another source to help craft your response. While the short answer questions are not necessarily being assessed on grammatical correctness or sentence structure, do your best to communicate yourself clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Gradient Descent [Suggested Time: 20 min]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![best fit line](visuals/best_fit_line.png)\n",
    "\n",
    "The best fit line that goes through the scatterplot up above can be generalized in the following equation: $$y = mx + b$$\n",
    "\n",
    "Of all the possible lines, we can prove why that particular line was chosen using the plot down below:\n",
    "\n",
    "![](visuals/cost_curve.png)\n",
    "\n",
    "where RSS is defined as the residual sum of squares:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "RSS &= \\sum_{i=1}^n(actual - expected)^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y})^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - (mx_i + b))^2\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) What is a more generalized name for the RSS curve above? How could a machine learning model use this curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "\n",
    "The more generalized name for the RSS curve above is the cost function.  \n",
    "A machine learning model could use gradient descent to find the parameters that would output the minimal cost (error). \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Would you rather choose a $m$ value of 0.08 or 0.05 from the RSS curve up above? Explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYour written answer here\\n\\nI would rather choose a m value of .05 from the RSS curve above because around .05 is the lowest point on the cost function. \\nAt this point, the value of RSS appears to no longer decrease meaning that at this point, loss would be at its minimum.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "\n",
    "I would rather choose a m value of .05 from the RSS curve above because around .05 is the lowest point on the cost function. \n",
    "At this point, the value of RSS appears to no longer decrease meaning that at this point, loss would be at its minimum.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](visuals/gd.png)\n",
    "\n",
    "### 1.3) Using the gradient descent visual from above, explain why the distance between estimates in each step is getting smaller as more steps occur with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "The step sizes are getting smaller towards the bottom of the curve because the slope at the bottom is smaller. Slope helps determine stepsize.  \n",
    "So, in the beginning slope is larger and steps are bigger.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) What does the learning rate do in the gradient descent algorithm? Explain how a very small and a very large learning rate would affect the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "The learning rate is how big of a step we want to take when moving down the cost curve to find the minimum loss point.  The learning rate is a scaler within the gradient descent algorithm. \n",
    "A very small learning rate would take forever to reach the minimum/optimization point.  A very large learning rate could overstep the minimum and thus never find the precise minimum loss point.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Logistic Regression [Suggested Time: 15 min]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Why is logistic regression typically better than linear regession for modeling a binary target/outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "Logistic regression is better than linear regression for modeling a binary target because it uses the sigmoid function \"s\" curve to classify the data and predict probabilities. Linear's use of OLS doesn't adequately describe binary data since there is not a 'linear relationship', and linear doesn't predict binary targets but instead the value of continuous variables. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) What is one advantage that logistic regression can have over other classification methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "Logistic regression can be really efficient as it outputs the probabilities of a specific class and these probabilities can be used for class predictions.  Logistric Regression is really interpretable and its training/prediction are fast. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Classification Metrics [Suggested Time: 20 min]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnf matrix](visuals/cnf_matrix.png)\n",
    "\n",
    "### 3.1) Using the confusion matrix above, calculate precision, recall, and F-1 score.\n",
    "\n",
    "Show your work, not just your final numeric answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here to calculate precision\n",
    "TP=54 \n",
    "TN=30\n",
    "FP=12\n",
    "FN=4\n",
    "\n",
    "Precision = (TP)/(TP+FP)\n",
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9310344827586207"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here to calculate recall\n",
    "Recall = (TP)/(TP+FN)\n",
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709677419354839"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here to calculate F-1 score\n",
    "F1=2*((Precision*Recall)/(Precision+Recall))\n",
    "F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"visuals/many_roc.png\" width = \"700\">\n",
    "\n",
    "### 3.2) Which ROC curve from the above graph is the best? Explain your reasoning. \n",
    "\n",
    "Note: each ROC curve represents one model, each labeled with the feature(s) inside each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "The pink ROC curve is the best.  \n",
    "The AUC (area under the ROC) is biggest under the pink curve, which tells us that the pink curve (model) performs the best across all possible classification thresholds so the pink model fits best overall, regardless of the threshold used.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Example\n",
    "\n",
    "The following cell includes code to train and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier has an accuracy score of 0.956.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Include relevant imports\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "network_df = pickle.load(open('write_data/sample_network_data.pkl', 'rb'))\n",
    "\n",
    "# partion features and target \n",
    "X = network_df.drop('Purchased', axis=1)\n",
    "y = network_df['Purchased']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n",
    "\n",
    "# scale features\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# build classifier\n",
    "model = LogisticRegression(C=1e5, solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# get the accuracy score\n",
    "print(f'The classifier has an accuracy score of {round(accuracy_score(y_test, y_test_pred), 3)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Explain how the distribution of `y` shown below could explain the very high accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    257\n",
       "1     13\n",
       "Name: Purchased, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9518518518518518"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "257/270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "\n",
    "The distribution of y is extremely unbalanced, the 0 class makes up 95% of our target data.  \n",
    "Therefore, the very high accuracy score doesn't really vary much from just predicting the 0 class which is the majority class.  \n",
    "And so here the accuracy score doesn't actually tell us the predictive power of the model and what type of errors the model is making.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) What method could you use to address the issue discovered in Question 3.3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "The classes could be balanced using class imbalanced methods such as oversampling/resampling so that the distribution of Y is more even.  Then, accuracy could give us a better picture.\n",
    "Additionally, we could also use a different metric to evaluate the model, such as F1 score.  F1 gives us a balance of precision and recall.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Decision Trees [Suggested Time: 20 min]\n",
    "---\n",
    "\n",
    "### Concepts \n",
    "You're given a dataset of **30** elements, 15 of which belong to a positive class (denoted by *`+`* ) and 15 of which do not (denoted by `-`). These elements are described by two attributes, A and B, that can each have either one of two values, true or false. \n",
    "\n",
    "The diagrams below show the result of splitting the dataset by attribute: the diagram on the left hand side shows that if we split by attribute A there are 13 items of the positive class and 2 of the negative class in one branch and 2 of the positive and 13 of the negative in the other branch. The right hand side shows that if we split the data by attribute B there are 8 items of the positive class and 7 of the negative class in one branch and 7 of the positive and 8 of the negative in the other branch.\n",
    "\n",
    "<img src=\"visuals/decision_stump.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Which one of the two attributes resulted in the best split of the original data? How do you select the best attribute to split a tree at each node? \n",
    "\n",
    "It may be helpful to discuss splitting criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "Attribute A resulted in the best split of the original data. \n",
    "Attribute A resulted in a higher information gain (and decrease in entropy), meaning more homogenous (pure) branches were produced. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Example\n",
    "\n",
    "In this section, you will use decision trees to fit a classification model to the wine dataset. The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Relevant imports \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the data \n",
    "wine = load_wine()\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X = pd.DataFrame(X, columns=wine.feature_names)\n",
    "y = pd.Series(y)\n",
    "y.name = 'target'\n",
    "df = pd.concat([X, y.to_frame()], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "# Get the shape of the DataFrame \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    71\n",
       "0    59\n",
       "2    48\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "# Get the distribution of the target variable \n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Split the data into training and test sets. Create training and test sets with `test_size=0.5` and `random_state=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Fit a decision tree model with scikit-learn to the training data. Use parameter defaults, except for `random_state=1`. Use the fitted classifier to generate predictions for the test data.\n",
    "\n",
    "You can use the Scikit-learn DecisionTreeClassifier (docs [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(random_state=1)\n",
    "#training the model\n",
    "dtc.fit(X_train,y_train)\n",
    "#predicting on test set\n",
    "y_pred_test=dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4) Obtain the accuracy score of the predictions on the test set. \n",
    "\n",
    "You can use the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8764044943820225\n"
     ]
    }
   ],
   "source": [
    "# Your code imports here\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# Replace None with appropriate code \n",
    "\n",
    "print('Accuracy Score:',metrics.accuracy_score(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5) Produce a confusion matrix for the predictions on the test set. \n",
    "\n",
    "You can use the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  6,  0],\n",
       "       [ 2, 30,  2],\n",
       "       [ 0,  1, 21]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code imports here\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# Your code here \n",
    "CM=metrics.confusion_matrix(y_test,y_pred_test)\n",
    "CM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6) Do the accuracy score or confusion matrix reveal any substantial problems with this model's performance? Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your written answer here\n",
    "No, the accuracy score and confusion matrix do not reveal any substantial problems.  Most predictions reflect the actual values as shown on the diagonal of the confusion matrix.   \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
